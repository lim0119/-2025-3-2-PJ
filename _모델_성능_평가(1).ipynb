{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdtMUPBJwnrrPrRD2oyOwI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xBzYvhhd4LV"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# ğŸ“Œ ë‡Œ MRI ê¸°ë°˜ ì•Œì¸ í•˜ì´ë¨¸ ë³´ì¡°ì§„ë‹¨ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€(1)\n",
        "# ================================\n",
        "\n",
        "# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install scikit-learn matplotlib seaborn pandas numpy\n",
        "\n",
        "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "# ì„±ëŠ¥ì§€í‘œ ê³„ì‚°(sklearn), ë°ì´í„°ì²˜ë¦¬(pandas, numpy), ì‹œê°í™”(matplotlib, seaborn)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, precision_recall_curve, auc, confusion_matrix, f1_score,\n",
        "    balanced_accuracy_score, matthews_corrcoef, brier_score_loss, roc_curve\n",
        ")\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 2. CSV íŒŒì¼ ì—…ë¡œë“œ\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # ì—…ë¡œë“œí•  CSV ì„ íƒ(CSV=ë°ì´í„°ì…‹)\n",
        "\n",
        "# ì—…ë¡œë“œí•œ íŒŒì¼ëª… ê°€ì ¸ì˜¤ê¸°\n",
        "filename = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(filename)\n",
        "print(\"Columns in dataset:\", df.columns.tolist())\n",
        "\n",
        "# 3. ì»¬ëŸ¼ ì„¤ì • (í•„ìˆ˜/ì„ íƒ)\n",
        "# ë°˜ë“œì‹œ í•„ìš”í•œ ì»¬ëŸ¼: ì‹¤ì œ ë¼ë²¨(label), ëª¨ë¸ ì˜ˆì¸¡ í™•ë¥ (pred_prob)\n",
        "# ì¶”ê°€ì ìœ¼ë¡œ í™˜ì ID, ì‹œì (timepoint), subgroup ìˆìœ¼ë©´ ë¶„ì„ í™•ì¥ ê°€ëŠ¥\n",
        "y_true = df['label'].values            # ì‹¤ì œ ë¼ë²¨\n",
        "y_pred_prob = df['pred_prob'].values   # ëª¨ë¸ ì˜ˆì¸¡ í™•ë¥  (0~1)\n",
        "\n",
        "patient_id = df['patient_id'].values if 'patient_id' in df.columns else None\n",
        "timepoint = df['timepoint'].values if 'timepoint' in df.columns else None\n",
        "subgroup = df['subgroup'].values if 'subgroup' in df.columns else None\n",
        "\n",
        "# ================================\n",
        "# 4. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°(AUC / PR-AUC)\n",
        "#  ëª¨ë¸ ì„±ëŠ¥ì„ ìˆ˜ì¹˜ë¡œ í™•ì¸ (AUC, PR-AUC, Sensitivity, Specificity ë“±)\n",
        "# - AUC: ëª¨ë¸ì´ í™˜ì/ë¹„í™˜ì êµ¬ë¶„ì„ ì–¼ë§ˆë‚˜ ì˜í•˜ëŠ”ì§€\n",
        "# - PR-AUC: ë¶ˆê· í˜• ë°ì´í„°(í™˜ì ìˆ˜ ì ìŒ)ì— ì í•©í•œ ì§€í‘œ\n",
        "# - Sensitivity(ë¯¼ê°ë„): í™˜ìë¥¼ ì˜ ì¡ì•„ë‚´ëŠ” ë¹„ìœ¨\n",
        "# - Specificity(íŠ¹ì´ë„): ì •ìƒì¸ì„ ì˜ êµ¬ë¶„í•˜ëŠ” ë¹„ìœ¨\n",
        "# - F1: Precisionê³¼ Recallì˜ ì¡°í™” í‰ê· \n",
        "auc_score = roc_auc_score(y_true, y_pred_prob)\n",
        "precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)\n",
        "pr_auc_score = auc(recall, precision)\n",
        "print(f\"AUC: {auc_score:.3f}, PR-AUC: {pr_auc_score:.3f}\")\n",
        "\n",
        "# Threshold ê¸°ë°˜ ì§€í‘œ\n",
        "threshold = 0.5  # ì„ê³„ê°’ (0.5 ì´ìƒì´ë©´ ì•Œì¸ í•˜ì´ë¨¸ë¡œ ë¶„ë¥˜)\n",
        "y_pred_label = (y_pred_prob >= threshold).astype(int)\n",
        "cm = confusion_matrix(y_true, y_pred_label)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "sensitivity = tp / (tp + fn) if (tp+fn) > 0 else 0\n",
        "specificity = tn / (tn + fp) if (tn+fp) > 0 else 0\n",
        "f1 = f1_score(y_true, y_pred_label)\n",
        "print(f\"Sensitivity: {sensitivity:.3f}, Specificity: {specificity:.3f}, F1: {f1:.3f}\")\n",
        "\n",
        "# ì¶”ê°€ ì§€í‘œ\n",
        "# - Balanced Accuracy: ë¯¼ê°ë„+íŠ¹ì´ë„ í‰ê· \n",
        "# - MCC: ì–‘ì„±/ìŒì„± ê· í˜• ê³ ë ¤í•œ ìƒê´€ê³„ìˆ˜\n",
        "# - Brier Score: ì˜ˆì¸¡í™•ë¥ ê³¼ ì‹¤ì œ ì •ë‹µì˜ ì°¨ì´\n",
        "bal_acc = balanced_accuracy_score(y_true, y_pred_label)\n",
        "mcc = matthews_corrcoef(y_true, y_pred_label)\n",
        "brier = brier_score_loss(y_true, y_pred_prob)\n",
        "print(f\"Balanced Acc: {bal_acc:.3f}, MCC: {mcc:.3f}, Brier Score: {brier:.3f}\")\n",
        "\n",
        "# ================================\n",
        "# 5. ë¶€íŠ¸ìŠ¤íŠ¸ë© 95% CI ê³„ë‹¨\n",
        "# ìƒ˜í”Œì„ ì—¬ëŸ¬ ë²ˆ ì¬ì¶”ì¶œí•´ì„œ(bootstrap) ì§€í‘œì˜ ì‹ ë¢°êµ¬ê°„ ê³„ì‚°\n",
        "# ë‹¨ìˆœ ì ìˆ˜ë¿ë§Œ ì•„ë‹ˆë¼ \"ë¶ˆí™•ì‹¤ì„±\" ë²”ìœ„ë„ í™•ì¸ ê°€ëŠ¥\n",
        "def bootstrap_ci(y_true, y_pred_prob, n_bootstrap=1000, alpha=0.05):\n",
        "    rng = np.random.default_rng(seed=42)\n",
        "    metrics_dict = {\"AUC\": [], \"PR-AUC\": [], \"Sensitivity\": [],\n",
        "                    \"Specificity\": [], \"F1\": [], \"Balanced Acc\": []}\n",
        "\n",
        "    for _ in range(n_bootstrap):\n",
        "        idx = rng.choice(len(y_true), len(y_true), replace=True)\n",
        "        y_true_bs = y_true[idx]\n",
        "        y_prob_bs = y_pred_prob[idx]\n",
        "        y_label_bs = (y_prob_bs >= 0.5).astype(int)\n",
        "\n",
        "        if len(np.unique(y_true_bs)) < 2:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            auc_bs = roc_auc_score(y_true_bs, y_prob_bs)\n",
        "            precision_bs, recall_bs, _ = precision_recall_curve(y_true_bs, y_prob_bs)\n",
        "            pr_auc_bs = auc(recall_bs, precision_bs)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        cm_bs = confusion_matrix(y_true_bs, y_label_bs)\n",
        "        if cm_bs.shape == (2,2):\n",
        "            tn, fp, fn, tp = cm_bs.ravel()\n",
        "            sens_bs = tp / (tp+fn) if (tp+fn) > 0 else 0\n",
        "            spec_bs = tn / (tn+fp) if (tn+fp) > 0 else 0\n",
        "        else:\n",
        "            sens_bs, spec_bs = np.nan, np.nan\n",
        "\n",
        "        f1_bs = f1_score(y_true_bs, y_label_bs)\n",
        "        bal_bs = balanced_accuracy_score(y_true_bs, y_label_bs)\n",
        "\n",
        "        metrics_dict[\"AUC\"].append(auc_bs)\n",
        "        metrics_dict[\"PR-AUC\"].append(pr_auc_bs)\n",
        "        metrics_dict[\"Sensitivity\"].append(sens_bs)\n",
        "        metrics_dict[\"Specificity\"].append(spec_bs)\n",
        "        metrics_dict[\"F1\"].append(f1_bs)\n",
        "        metrics_dict[\"Balanced Acc\"].append(bal_bs)\n",
        "\n",
        "    # CI ê³„ì‚°\n",
        "    ci_results = {}\n",
        "    for metric, values in metrics_dict.items():\n",
        "        if len(values) > 0:\n",
        "            lower = np.percentile(values, 100*alpha/2)\n",
        "            upper = np.percentile(values, 100*(1-alpha/2))\n",
        "            ci_results[metric] = (np.mean(values), lower, upper)\n",
        "    return ci_results\n",
        "\n",
        "ci_results = bootstrap_ci(y_true, y_pred_prob)\n",
        "print(\"\\nğŸ“Œ Bootstrap 95% CI ê²°ê³¼\")\n",
        "for metric, (mean, lower, upper) in ci_results.items():\n",
        "    print(f\"{metric}: {mean:.3f} (95% CI {lower:.3f} ~ {upper:.3f})\")\n",
        "\n",
        "# ================================\n",
        "# 6. ì‹œê°í™”\n",
        "# ROC, PR, Confusion Matrix, Calibration Curve ì‹œê°í™”\n",
        "# - ROC: ëª¨ë¸ì´ í™˜ì/ë¹„í™˜ì êµ¬ë¶„í•˜ëŠ” ëŠ¥ë ¥\n",
        "# - PR Curve: ì†Œìˆ˜ í´ë˜ìŠ¤(í™˜ì)ì— ëŒ€í•œ ëª¨ë¸ ì„±ëŠ¥ ê°•ì¡°\n",
        "# - Confusion Matrix: ì‹¤ì œ/ì˜ˆì¸¡ ë¹„êµ (ì˜¤ë¶„ë¥˜ í™•ì¸)\n",
        "# - Calibration Curve: ì˜ˆì¸¡ í™•ë¥ ê³¼ ì‹¤ì œ ë°œìƒë¥  ì¼ì¹˜ ì—¬ë¶€ í™•ì¸\n",
        "fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC (AUC={auc_score:.3f})\")\n",
        "plt.plot([0,1],[0,1],'--',color='gray')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# PR Curve\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, label=f\"PR-AUC={pr_auc_score:.3f}\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix heatmap\n",
        "plt.figure()\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"NC\",\"AD\"], yticklabels=[\"NC\",\"AD\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Calibration Curve\n",
        "prob_true, prob_pred = calibration_curve(y_true, y_pred_prob, n_bins=10)\n",
        "plt.figure()\n",
        "plt.plot(prob_pred, prob_true, marker='o')\n",
        "plt.plot([0,1],[0,1],'--',color='gray')\n",
        "plt.xlabel(\"Predicted Probability\")\n",
        "plt.ylabel(\"Observed Frequency\")\n",
        "plt.title(\"Calibration Curve\")\n",
        "plt.show()\n",
        "\n",
        "# ================================\n",
        "# 7. Calibration Error (ECE)\n",
        "# ì˜ˆì¸¡ í™•ë¥ ì´ ì‹¤ì œì™€ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ ìˆ˜ì¹˜í™”\n",
        "# (ë‚®ì„ìˆ˜ë¡ ëª¨ë¸ì˜ í™•ë¥ ê°’ ì‹ ë¢° ê°€ëŠ¥)\n",
        "bin_counts, _ = np.histogram(y_pred_prob, bins=10)\n",
        "ece = np.sum(bin_counts / len(y_pred_prob) * np.abs(prob_true - prob_pred))\n",
        "print(f\"ECE: {ece:.3f}\")\n",
        "\n",
        "# ================================\n",
        "# 8. Uncertainty (ì˜ˆì‹œ)\n",
        "# ë¶ˆí™•ì‹¤ì„± ê°’ ì˜ˆì‹œ (ì‹¤ì œë¡œëŠ” Bayesian, MC Dropout ë“± í•„ìš”)\n",
        "# - ëª¨ë¸ì´ ìì‹  ì—†ëŠ” ìƒ˜í”Œ(high uncertainty) íƒì§€ ê°€ëŠ¥\n",
        "uncertainty = np.random.rand(len(y_pred_prob)) * 0.1\n",
        "high_uncertainty_idx = np.where(uncertainty > 0.08)[0]\n",
        "print(f\"High uncertainty samples: {high_uncertainty_idx}\")\n",
        "\n",
        "# ================================\n",
        "# 9. Subgroup ì„±ëŠ¥\n",
        "# íŠ¹ì • ê·¸ë£¹(ì„±ë³„, ë‚˜ì´ëŒ€ ë“±)ë³„ ì„±ëŠ¥ ì°¨ì´ ë¶„ì„\n",
        "# - ê³µì •ì„±/í¸í–¥ì„± í™•ì¸\n",
        "if subgroup is not None:\n",
        "    df_sub = pd.DataFrame({'y_true':y_true, 'y_pred_prob':y_pred_prob, 'subgroup':subgroup})\n",
        "    for g in df_sub['subgroup'].unique():\n",
        "        g_df = df_sub[df_sub['subgroup']==g]\n",
        "        if len(g_df['y_true'].unique()) > 1:\n",
        "            g_auc = roc_auc_score(g_df['y_true'], g_df['y_pred_prob'])\n",
        "            print(f\"Subgroup {g} AUC: {g_auc:.3f}\")\n",
        "\n",
        "# ================================\n",
        "# 10. Longitudinal Consistency\n",
        "# í•œ í™˜ìë¥¼ ì‹œê°„(timepoint) ë”°ë¼ ì¶”ì  â†’ ì˜ˆì¸¡ê°’ì˜ ì¼ê´€ì„± ì²´í¬\n",
        "# - ê°‘ìê¸° í° ë³€í™”ê°€ ìˆìœ¼ë©´ ëª¨ë¸ ì•ˆì •ì„± ë¬¸ì œ ê°€ëŠ¥\n",
        "if patient_id is not None and timepoint is not None:\n",
        "    df_long = pd.DataFrame({'patient_id':patient_id, 'timepoint':timepoint, 'y_pred_prob':y_pred_prob})\n",
        "    for pid in df_long['patient_id'].unique():\n",
        "        series = df_long[df_long['patient_id']==pid].sort_values('timepoint')['y_pred_prob'].values\n",
        "        if len(series) > 1:\n",
        "            change = np.diff(series)\n",
        "            if np.any(np.abs(change) > 0.5):\n",
        "                print(f\"Patient {pid} has abrupt prediction change: {series}\")\n",
        "\n",
        "# ================================\n",
        "# 11. Quality Check (QC)\n",
        "# ë°ì´í„° í’ˆì§ˆ ê²€ì¦\n",
        "# - ê²°ì¸¡ì¹˜, 0~1 ë²”ìœ„ ë°– í™•ë¥ ê°’ ì—¬ë¶€ í™•ì¸\n",
        "df_qc = pd.DataFrame({'y_pred_prob':y_pred_prob})\n",
        "missing_count = df_qc.isna().sum().values[0]\n",
        "out_of_bounds = np.sum((df_qc['y_pred_prob']<0) | (df_qc['y_pred_prob']>1))\n",
        "print(f\"Missing values: {missing_count}, Out-of-bounds predictions: {out_of_bounds}\")\n"
      ]
    }
  ]
}