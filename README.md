# -2025-3-2-PJ
# ğŸ“Œ ë‡Œ MRI ê¸°ë°˜ ì•Œì¸ í•˜ì´ë¨¸ ë³´ì¡°ì§„ë‹¨ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€(1)


# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
!pip install scikit-learn matplotlib seaborn pandas numpy

# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
# ì„±ëŠ¥ì§€í‘œ ê³„ì‚°(sklearn), ë°ì´í„°ì²˜ë¦¬(pandas, numpy), ì‹œê°í™”(matplotlib, seaborn)
import numpy as np
import pandas as pd
from sklearn.metrics import (
    roc_auc_score, precision_recall_curve, auc, confusion_matrix, f1_score,
    balanced_accuracy_score, matthews_corrcoef, brier_score_loss, roc_curve
)
from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt
import seaborn as sns

# 2. CSV íŒŒì¼ ì—…ë¡œë“œ
from google.colab import files
uploaded = files.upload()  # ì—…ë¡œë“œí•  CSV ì„ íƒ(CSV=ë°ì´í„°ì…‹)

# ì—…ë¡œë“œí•œ íŒŒì¼ëª… ê°€ì ¸ì˜¤ê¸°
filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)
print("Columns in dataset:", df.columns.tolist())

# 3. ì»¬ëŸ¼ ì„¤ì • (í•„ìˆ˜/ì„ íƒ)
# ë°˜ë“œì‹œ í•„ìš”í•œ ì»¬ëŸ¼: ì‹¤ì œ ë¼ë²¨(label), ëª¨ë¸ ì˜ˆì¸¡ í™•ë¥ (pred_prob)
# ì¶”ê°€ì ìœ¼ë¡œ í™˜ì ID, ì‹œì (timepoint), subgroup ìˆìœ¼ë©´ ë¶„ì„ í™•ì¥ ê°€ëŠ¥
y_true = df['label'].values            # ì‹¤ì œ ë¼ë²¨
y_pred_prob = df['pred_prob'].values   # ëª¨ë¸ ì˜ˆì¸¡ í™•ë¥  (0~1)

patient_id = df['patient_id'].values if 'patient_id' in df.columns else None
timepoint = df['timepoint'].values if 'timepoint' in df.columns else None
subgroup = df['subgroup'].values if 'subgroup' in df.columns else None

# ================================
# 4. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°(AUC / PR-AUC)
#  ëª¨ë¸ ì„±ëŠ¥ì„ ìˆ˜ì¹˜ë¡œ í™•ì¸ (AUC, PR-AUC, Sensitivity, Specificity ë“±)
# - AUC: ëª¨ë¸ì´ í™˜ì/ë¹„í™˜ì êµ¬ë¶„ì„ ì–¼ë§ˆë‚˜ ì˜í•˜ëŠ”ì§€
# - PR-AUC: ë¶ˆê· í˜• ë°ì´í„°(í™˜ì ìˆ˜ ì ìŒ)ì— ì í•©í•œ ì§€í‘œ
# - Sensitivity(ë¯¼ê°ë„): í™˜ìë¥¼ ì˜ ì¡ì•„ë‚´ëŠ” ë¹„ìœ¨
# - Specificity(íŠ¹ì´ë„): ì •ìƒì¸ì„ ì˜ êµ¬ë¶„í•˜ëŠ” ë¹„ìœ¨
# - F1: Precisionê³¼ Recallì˜ ì¡°í™” í‰ê· 
auc_score = roc_auc_score(y_true, y_pred_prob)
precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)
pr_auc_score = auc(recall, precision)
print(f"AUC: {auc_score:.3f}, PR-AUC: {pr_auc_score:.3f}")

# Threshold ê¸°ë°˜ ì§€í‘œ
threshold = 0.5  # ì„ê³„ê°’ (0.5 ì´ìƒì´ë©´ ì•Œì¸ í•˜ì´ë¨¸ë¡œ ë¶„ë¥˜)
y_pred_label = (y_pred_prob >= threshold).astype(int)
cm = confusion_matrix(y_true, y_pred_label)
tn, fp, fn, tp = cm.ravel()

sensitivity = tp / (tp + fn) if (tp+fn) > 0 else 0
specificity = tn / (tn + fp) if (tn+fp) > 0 else 0
f1 = f1_score(y_true, y_pred_label)
print(f"Sensitivity: {sensitivity:.3f}, Specificity: {specificity:.3f}, F1: {f1:.3f}")

# ì¶”ê°€ ì§€í‘œ
# - Balanced Accuracy: ë¯¼ê°ë„+íŠ¹ì´ë„ í‰ê· 
# - MCC: ì–‘ì„±/ìŒì„± ê· í˜• ê³ ë ¤í•œ ìƒê´€ê³„ìˆ˜
# - Brier Score: ì˜ˆì¸¡í™•ë¥ ê³¼ ì‹¤ì œ ì •ë‹µì˜ ì°¨ì´
bal_acc = balanced_accuracy_score(y_true, y_pred_label)
mcc = matthews_corrcoef(y_true, y_pred_label)
brier = brier_score_loss(y_true, y_pred_prob)
print(f"Balanced Acc: {bal_acc:.3f}, MCC: {mcc:.3f}, Brier Score: {brier:.3f}")

# ================================
# 5. ë¶€íŠ¸ìŠ¤íŠ¸ë© 95% CI ê³„ë‹¨
# ìƒ˜í”Œì„ ì—¬ëŸ¬ ë²ˆ ì¬ì¶”ì¶œí•´ì„œ(bootstrap) ì§€í‘œì˜ ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
# ë‹¨ìˆœ ì ìˆ˜ë¿ë§Œ ì•„ë‹ˆë¼ "ë¶ˆí™•ì‹¤ì„±" ë²”ìœ„ë„ í™•ì¸ ê°€ëŠ¥
def bootstrap_ci(y_true, y_pred_prob, n_bootstrap=1000, alpha=0.05):
    rng = np.random.default_rng(seed=42)
    metrics_dict = {"AUC": [], "PR-AUC": [], "Sensitivity": [], 
                    "Specificity": [], "F1": [], "Balanced Acc": []}

    for _ in range(n_bootstrap):
        idx = rng.choice(len(y_true), len(y_true), replace=True)
        y_true_bs = y_true[idx]
        y_prob_bs = y_pred_prob[idx]
        y_label_bs = (y_prob_bs >= 0.5).astype(int)

        if len(np.unique(y_true_bs)) < 2:
            continue

        try:
            auc_bs = roc_auc_score(y_true_bs, y_prob_bs)
            precision_bs, recall_bs, _ = precision_recall_curve(y_true_bs, y_prob_bs)
            pr_auc_bs = auc(recall_bs, precision_bs)
        except:
            continue

        cm_bs = confusion_matrix(y_true_bs, y_label_bs)
        if cm_bs.shape == (2,2):
            tn, fp, fn, tp = cm_bs.ravel()
            sens_bs = tp / (tp+fn) if (tp+fn) > 0 else 0
            spec_bs = tn / (tn+fp) if (tn+fp) > 0 else 0
        else:
            sens_bs, spec_bs = np.nan, np.nan

        f1_bs = f1_score(y_true_bs, y_label_bs)
        bal_bs = balanced_accuracy_score(y_true_bs, y_label_bs)

        metrics_dict["AUC"].append(auc_bs)
        metrics_dict["PR-AUC"].append(pr_auc_bs)
        metrics_dict["Sensitivity"].append(sens_bs)
        metrics_dict["Specificity"].append(spec_bs)
        metrics_dict["F1"].append(f1_bs)
        metrics_dict["Balanced Acc"].append(bal_bs)

    # CI ê³„ì‚°
    ci_results = {}
    for metric, values in metrics_dict.items():
        if len(values) > 0:
            lower = np.percentile(values, 100*alpha/2)
            upper = np.percentile(values, 100*(1-alpha/2))
            ci_results[metric] = (np.mean(values), lower, upper)
    return ci_results

ci_results = bootstrap_ci(y_true, y_pred_prob)
print("\nğŸ“Œ Bootstrap 95% CI ê²°ê³¼")
for metric, (mean, lower, upper) in ci_results.items():
    print(f"{metric}: {mean:.3f} (95% CI {lower:.3f} ~ {upper:.3f})")

# ================================
# 6. ì‹œê°í™”
# ROC, PR, Confusion Matrix, Calibration Curve ì‹œê°í™”
# - ROC: ëª¨ë¸ì´ í™˜ì/ë¹„í™˜ì êµ¬ë¶„í•˜ëŠ” ëŠ¥ë ¥
# - PR Curve: ì†Œìˆ˜ í´ë˜ìŠ¤(í™˜ì)ì— ëŒ€í•œ ëª¨ë¸ ì„±ëŠ¥ ê°•ì¡°
# - Confusion Matrix: ì‹¤ì œ/ì˜ˆì¸¡ ë¹„êµ (ì˜¤ë¶„ë¥˜ í™•ì¸)
# - Calibration Curve: ì˜ˆì¸¡ í™•ë¥ ê³¼ ì‹¤ì œ ë°œìƒë¥  ì¼ì¹˜ ì—¬ë¶€ í™•ì¸
fpr, tpr, _ = roc_curve(y_true, y_pred_prob)
plt.figure()
plt.plot(fpr, tpr, label=f"ROC (AUC={auc_score:.3f})")
plt.plot([0,1],[0,1],'--',color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

# PR Curve
plt.figure()
plt.plot(recall, precision, label=f"PR-AUC={pr_auc_score:.3f}")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.show()

# Confusion Matrix heatmap
plt.figure()
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["NC","AD"], yticklabels=["NC","AD"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

# Calibration Curve
prob_true, prob_pred = calibration_curve(y_true, y_pred_prob, n_bins=10)
plt.figure()
plt.plot(prob_pred, prob_true, marker='o')
plt.plot([0,1],[0,1],'--',color='gray')
plt.xlabel("Predicted Probability")
plt.ylabel("Observed Frequency")
plt.title("Calibration Curve")
plt.show()

# ================================
# 7. Calibration Error (ECE)
# ì˜ˆì¸¡ í™•ë¥ ì´ ì‹¤ì œì™€ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ ìˆ˜ì¹˜í™”
# (ë‚®ì„ìˆ˜ë¡ ëª¨ë¸ì˜ í™•ë¥ ê°’ ì‹ ë¢° ê°€ëŠ¥)
bin_counts, _ = np.histogram(y_pred_prob, bins=10)
ece = np.sum(bin_counts / len(y_pred_prob) * np.abs(prob_true - prob_pred))
print(f"ECE: {ece:.3f}")

# ================================
# 8. Uncertainty (ì˜ˆì‹œ)
# ë¶ˆí™•ì‹¤ì„± ê°’ ì˜ˆì‹œ (ì‹¤ì œë¡œëŠ” Bayesian, MC Dropout ë“± í•„ìš”)
# - ëª¨ë¸ì´ ìì‹  ì—†ëŠ” ìƒ˜í”Œ(high uncertainty) íƒì§€ ê°€ëŠ¥
uncertainty = np.random.rand(len(y_pred_prob)) * 0.1
high_uncertainty_idx = np.where(uncertainty > 0.08)[0]
print(f"High uncertainty samples: {high_uncertainty_idx}")

# ================================
# 9. Subgroup ì„±ëŠ¥
# íŠ¹ì • ê·¸ë£¹(ì„±ë³„, ë‚˜ì´ëŒ€ ë“±)ë³„ ì„±ëŠ¥ ì°¨ì´ ë¶„ì„
# - ê³µì •ì„±/í¸í–¥ì„± í™•ì¸
if subgroup is not None:
    df_sub = pd.DataFrame({'y_true':y_true, 'y_pred_prob':y_pred_prob, 'subgroup':subgroup})
    for g in df_sub['subgroup'].unique():
        g_df = df_sub[df_sub['subgroup']==g]
        if len(g_df['y_true'].unique()) > 1:
            g_auc = roc_auc_score(g_df['y_true'], g_df['y_pred_prob'])
            print(f"Subgroup {g} AUC: {g_auc:.3f}")

# ================================
# 10. Longitudinal Consistency
# í•œ í™˜ìë¥¼ ì‹œê°„(timepoint) ë”°ë¼ ì¶”ì  â†’ ì˜ˆì¸¡ê°’ì˜ ì¼ê´€ì„± ì²´í¬
# - ê°‘ìê¸° í° ë³€í™”ê°€ ìˆìœ¼ë©´ ëª¨ë¸ ì•ˆì •ì„± ë¬¸ì œ ê°€ëŠ¥
if patient_id is not None and timepoint is not None:
    df_long = pd.DataFrame({'patient_id':patient_id, 'timepoint':timepoint, 'y_pred_prob':y_pred_prob})
    for pid in df_long['patient_id'].unique():
        series = df_long[df_long['patient_id']==pid].sort_values('timepoint')['y_pred_prob'].values
        if len(series) > 1:
            change = np.diff(series)
            if np.any(np.abs(change) > 0.5):
                print(f"Patient {pid} has abrupt prediction change: {series}")

# ================================
# 11. Quality Check (QC)
# ë°ì´í„° í’ˆì§ˆ ê²€ì¦
# - ê²°ì¸¡ì¹˜, 0~1 ë²”ìœ„ ë°– í™•ë¥ ê°’ ì—¬ë¶€ í™•ì¸
df_qc = pd.DataFrame({'y_pred_prob':y_pred_prob})
missing_count = df_qc.isna().sum().values[0]
out_of_bounds = np.sum((df_qc['y_pred_prob']<0) | (df_qc['y_pred_prob']>1))
print(f"Missing values: {missing_count}, Out-of-bounds predictions: {out_of_bounds}")
